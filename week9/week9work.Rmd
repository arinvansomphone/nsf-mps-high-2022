---
title: "Week 9 Work"
author: "Arin Vansomphone"
date: "`r Sys.Date()`"
output: html_document
---
# Tidymodels: Get Started Notes
### Build a model
Loading packages:
```{r}
library(tidymodels)
library(readr)
library(broom.mixed)
library(dotwhisker)
```

Loading data:
```{r}
urchins <- read_csv("https://tidymodels.org/start/models/urchins.csv") %>%
  setNames(c("food_regime", "initial_volume", "width")) %>%
  mutate(food_regime = factor(food_regime, levels = c("Initial", "Low",
                                                      "High")))
urchins
```

Plotting the data:
```{r}
ggplot(urchins,
       aes(x = initial_volume,
           y = width,
           group = food_regime,
           col = food_regime)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7)
```

* ANOVA models work best with a continuous variable and a categorical variable.

Specifying a formula for an ANOVA model:
```{r}
width ~ initial_volume * food_regime
```

Specifying the functional form:
```{r}
linear_reg()
```

* The default linear regression option is `"lm"` for least squares. Check the `linear_reg()` documentation for more info.

Using the fit function:
```{r}
lm_mod <- linear_reg()
lm_fit <- lm_mod %>%
  fit(width ~ initial_volume * food_regime, data = urchins)
tidy(lm_fit) # tidying the output
```

Plotting a dot and whisker plot of our regression results:
```{r}
tidy(lm_fit) %>%
  dwplot(dot_args = list(size = 2, color = "black"),
         whisker_args = list(color = "black"),
         vline = geom_vline(xintercept = 0, color = "grey50", linetype = 2))
```

Generating new data to make predictions for:
```{r}
new_points <- expand.grid(initial_volume = 20,
                          food_regime = c("Initial", "Low", "High"))
new_points
```

Generating the mean body width values:
```{r}
mean_pred <- predict(lm_fit, new_data = new_points)
mean_pred
```

Plotting a mean graph (with error bars) of predictions:
```{r}
conf_int_pred <- predict(lm_fit,
                         new_data = new_points,
                         type = "conf_int")
conf_int_pred

plot_data <- 
  new_points %>%
  bind_cols(mean_pred) %>%
  bind_cols(conf_int_pred) 

ggplot(plot_data, aes(x = food_regime)) +
  geom_point(aes(y = .pred)) +
  geom_errorbar(aes(ymin = .pred_lower,
                    ymax = .pred_upper),
                width = .2) +
  labs(y = "urchin size")
```

A Bayesian analysis approach to modeling the data:
```{r}
library(rstanarm)

# set the prior distribution
prior_dist <- rstanarm::student_t(df = 1)

set.seed(123)

# make the parsnip model
bayes_mod <-
  linear_reg() %>%
  set_engine("stan",
             prior_intercept = prior_dist,
             prior = prior_dist)

# train the model
bayes_fit <-
  bayes_mod %>%
  fit(width ~ initial_volume * food_regime, data = urchins)

tidy(bayes_fit, conf.int = TRUE)
```

Plotting the Bayesian results:
```{r}
bayes_plot_data <- new_points %>%
  bind_cols(predict(bayes_fit, new_data = new_points)) %>%
  bind_cols(predict(bayes_fit, new_data = new_points, type = "conf_int"))

ggplot(bayes_plot_data, aes(x = food_regime)) +
  geom_point(aes(y = .pred)) +
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) +
  labs(y = "urchin size") +
  ggtitle("Bayesian model with t(1) prior distribution")
```

### Preprocess your data wit recipes
Loading packages:
```{r}
library(tidymodels)
library(nycflights13)
library(skimr)
```

Loading the data and modifying some variables:
```{r}
set.seed(123)

flight_data <- flights %>%
  mutate(
    arr_delay = ifelse(arr_delay >= 30, "late", "on_time"),
    arr_delay = factor(arr_delay),
    date = lubridate::as_date(time_hour)
  ) %>%
  inner_join(weather,by = c("origin", "time_hour")) %>%
  select(dep_time, flight, origin, dest, air_time, distance, carrier, date,
         arr_delay, time_hour) %>%
  na.omit() %>%
  mutate_if(is.character, as.factor) # character strings become factors
flight_data
```

Splitting the data into a training and testing set:
```{r}
library(rsample)
set.seed(222) # this enables reproducibility in random number generation

#put 3/4 of the data into the training set
data_split <- initial_split(flight_data, prop = 3/4)

train_data <- training(data_split)
test_data <- testing(data_split)
```

Creating the recipe:
```{r}
flights_rec <-
  recipe(arr_delay ~ ., data = train_data) %>%
  update_role(flight, time_hour, new_role = "ID") %>%
  step_date(date, features = c("dow", "month")) %>%
  step_holiday(date,
               holidays = timeDate::listHolidays("US"),
               keep_original_cols = FALSE) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())
summary(flights_rec)
```

Model specification:
```{r}
lr_mod <- logistic_reg() %>%
  set_engine("glm")
```

Bundle the model and recipe with data:
```{r}
flights_wflow <- workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(flights_rec)

flights_wflow
```

Training the model!
```{r}
flights_fit <- flights_wflow %>%
  fit(data = train_data)
```
```{r}
flights_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```

Running our model through the test data:
```{r}
flights_aug <- augment(flights_fit, test_data)

flights_aug %>%
  select(arr_delay, time_hour, flight, .pred_class, .pred_on_time)
```

Creating an ROC curve:
```{r}
flights_aug %>%
  roc_curve(truth = arr_delay, .pred_late) %>%
  autoplot()
```

Estimating the area under the curve:
```{r}
flights_aug %>%
  roc_auc(truth = arr_delay, .pred_late)
```

### Evaluate your model with resampling
Loading packages:
```{r}
library(tidymodels)
library(modeldata)
```

Loading data:
```{r}
data(cells, package = "modeldata")
cells
```

Removing the `case` variable to make our own split:
```{r}
set.seed(123)
cell_split <- initial_split(cells %>% select(-case),
                            strata = class) # stratified split
```

Train and test datasets:
```{r}
cell_train <- training(cell_split)
cell_test <- testing(cell_split)

cell_train %>%
  count(class) %>%
  mutate(prop = n/sum(n))
```

Creating a random forest model:
```{r}
rf_mod <- rand_forest(trees = 1000) %>%
  set_engine("ranger") %>%
  set_mode("classification")
```

Using the fit function:
```{r}
library(ranger)
set.seed(234)
rf_fit <- rf_mod %>%
  fit(class ~., data = cell_train)
rf_fit
```

Computing the area under ROC curve and overall accuracy (DO NOT DO IT ON TRAINING SET):
```{r}
rf_testing_pred <-
  predict(rf_fit, cell_test) %>%
  bind_cols(predict(rf_fit, cell_test, type = "prob")) %>%
  bind_cols(cell_test %>% select(class))
```
```{r}
rf_testing_pred %>%
  roc_auc(truth = class, .pred_PS)
```
```{r}
rf_testing_pred %>%
  accuracy(truth = class, .pred_class)
```

Resampling methods create series of data sets similar to the training/testing set.

Generating cross-validation resampling:
```{r}
set.seed(345)
folds <- vfold_cv(cell_train, v = 10)
folds
```

Resampling via the workflow:
```{r}
rf_wf <-
  workflow() %>%
  add_model(rf_mod) %>%
  add_formula(class ~ .)

set.seed(456)
rf_fit_rs <-
  rf_wf %>%
  fit_resamples(folds)
rf_fit_rs
```

Accuracy with `collect_metrics`:
```{r}
collect_metrics(rf_fit_rs)
```

### Tune model parameters
Loading packages:
```{r}
library(tidymodels)
library(rpart.plot)
library(vip)
```

Data:
```{r}
data(cells, package = "modeldata")
cells
```

Splitting the data:
```{r}
set.seed(123)
cell_split <- initial_split(cells %>% select(-case),
                            strata = class)
cell_train <- training(cell_split)
cell_test <- testing(cell_split)
```

Tuning hyperparameters:
```{r}
tune_spec <-
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>%
  set_engine("rpart") %>%
  set_mode("classification")
tune_spec
```

The hyperparameters need to be trained on many models using resampled data.
Creating a grid of values to use with convenience functions:
```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
tree_grid
```

Create CV folds for tuning:
```{r}
set.seed(234)
cell_folds <- vfold_cv(cell_train)
```

Actual tuning with `workflow()`:
```{r}
set.seed(345)

tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(class ~ .)

tree_res <- 
  tree_wf %>%
  tune_grid(
    resamples = cell_folds,
    grid = tree_grid
  )
tree_res
```

Plotting our results:
```{r}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

Selecting the best hyperparameter values:
```{r}
best_tree <- tree_res %>%
  select_best("accuracy")
best_tree
```

Updating the workflow:
```{r}
final_wf <- tree_wf %>%
  finalize_workflow(best_tree)
final_wf
```

The final fit:
```{r}
final_fit <- final_wf %>%
  last_fit(cell_split)

final_fit %>%
  collect_metrics()
```

We can also find out what variables were most important to the model:
```{r}
library(vip)

final_tree <- extract_workflow(final_fit)

final_tree %>%
  extract_fit_parsnip() %>%
  vip()
```

### A predictive modeling case study:
Loading packages:
```{r}
library(tidymodels)
library(readr)
library(vip)
```

Loading data:
```{r}
hotels <- read_csv('https://tidymodels.org/start/case-study/hotels.csv') %>%
  mutate(across(where(is.character), as.factor))

dim(hotels)
```

Splitting the data:
```{r}
set.seed(123)
splits <- initial_split(hotels, strata = children)

hotel_other <- training(splits)
hotel_test <- testing(splits)
```

Creating a validation set:
```{r}
set.seed(234)
val_set <- validation_split(hotel_other, strata = children, prop = 0.80)
val_set
```

First model: penalized logistic regression
```{r}
lr_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")
```

Putting together the recipe:
```{r}
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter",
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <-
  recipe(children ~ ., data = hotel_other) %>%
  step_date(arrival_date) %>%
  step_holiday(arrival_date, holidays = holidays) %>%
  step_rm(arrival_date) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())
```

Creating the workflow:
```{r}
lr_workflow <- workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(lr_recipe)
```

Creating a grid for tuning:
```{r}
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_reg_grid %>% top_n(-5) # lowest penalty values
```
```{r}
lr_reg_grid %>% top_n(5) # highest penalty values
```

Training against the grid values:
```{r}
library(glmnet)

lr_res <-
  lr_workflow %>%
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

Plotting validation set metrics:
```{r}
lr_res %>%
  collect_metrics() %>%
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  ylab("Area under the ROC curve") +
  scale_x_log10(labels = scales::label_number())
```

Selecting the best penalty value, which we've chosen to be 12:
```{r}
lr_best <- lr_res %>%
  collect_metrics() %>%
  arrange(penalty) %>%
  slice(12)

lr_auc <- lr_res %>%
  collect_predictions(parameters = lr_best) %>%
  roc_curve(children, .pred_children) %>%
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```

Second model: tree-based ensemble
Computing how many cores we have:
```{r}
cores <- parallel::detectCores()
cores
```

Creating the model:
```{r}
rf_mod <-
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("classification")
```

Creating the recipe (random forest models do not need dummy variables):
```{r}
rf_recipe <-
  recipe(children ~ ., data = hotel_other) %>%
  step_date(arrival_date) %>%
  step_holiday(arrival_date) %>%
  step_rm(arrival_date)
```

Creating the workflow:
```{r}
rf_workflow <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_recipe)
```

Training the model:
```{r}
set.seed(345)
rf_res <- rf_workflow %>%
  tune_grid(val_set, grid = 25, control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```



# Tidymodels in the 1,5 AG Project